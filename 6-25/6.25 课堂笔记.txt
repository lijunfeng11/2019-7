
课堂笔记：
 
 知识回顾：
 
 “用户画像”Hadoop项目：
 1、用户画像：
	在电商领域，用户在访问电商网站时，都会遗留他的访问痕迹（用户访问日志），商家
会对这些信息进行汇总统计，然后分析出每一位用户的行为特征（消费能力、个人爱好），
然后根据这些行为特征，对用户进行全方位的用户画像（分类），然后对用户进行智能的
商品推荐（把当前用户关注商品相似度高其他商品推荐该用户），实现最终的商家收益。

2、项目分为四大部分：
	第一部分：项目业务背景介绍
	第二部分：整体技术流程及架构
	第三部分：项目各模块开发
	第四部分：大数据数据可视化
	
3、项目业务背景介绍
	见图
	
4、项目模块划分：
	（1）数据采集；
			1）技术框架：flume
				一个flume的对象叫做agent , 以下三个组件属于agent对象的三个属性；
				1）source  : 接收器，负责接收数据的组件；
				2）channel ： 数据传输通道，将source的数据传输给sinkl
				3）sink : 发送器，负责将数据发送到目的地；
			2）对于flume的开发，一般情况只需要配置脚本即可。
				（1）命名空间；
				（2）配置source的类型和参数；
				（3）配置channel的类型和参数；
				（4）配置sink的类型和参数；
				（5）将三个属性进行封装，形成agent对象 
			3）项目中flume的使用：
				  在本次项目中flume监听服务器上的一个日志文件，当该日志文件有新数据
				 产生时，flume则把该数据获取，然后发送到HDFS上存储；
				 
	（2）数据的预处理（ETL : 数据清洗）：
			数据预处理也叫做ETL（数据清洗），将历史数据中，和当前业务无关的数据过滤掉
		只保留业务相关数据。
			在数据预处理阶段：
				（1）对历史数据进行数据清洗；
				（2）在数据清洗的基础之上，封装“页面”模型（就是把用户访问的一个页面中
				所包含的信息进行封装）；
				（3）在“页面”模型的基础之上，封装“会话”模型（就是将一个用户访问的所有页面信息，
			进行汇总，从而整体对用户进行分析）；
			
				1）上传access.log文件到HDFS；
				2）将数据清洗代码进行打包，然后上传到Linux, 
				3）通过命令运行当前程序；
				hadoop jar project01.jar com.webclick.mr.pre.WeblogPreProcess /access.log /output
				
				
				
	（3）数据仓库设计与开发（把清洗后的数据入库，然后使用HQL语句进行分析）：
	（4）业务指标统计（与专业有关，PV , UV）：
	（5）数据迁移（HDFS ---> Mysql）；
	（6）数据可视化展示（以报表的方式，将数据展现给用户）；
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

 
 